{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 1 import libaries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prob 1 explore data\n",
    "\n",
    "customer_churn = pd.read_csv('Telco-Customer-Churn.csv')\n",
    "print(customer_churn.head())\n",
    "# Display summary statistics\n",
    "print(customer_churn.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prob 1 Clean data\n",
    "def load_data():\n",
    "    # Load the data\n",
    "    telco = pd.read_csv(\"Telco-Customer-Churn.csv\")\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    telco.drop(columns=['customerID', 'PaymentMethod'], inplace=True)\n",
    "\n",
    "    # Filter rows\n",
    "    telco = telco[(telco['InternetService'] != \"No\") & (telco['PhoneService'] != \"No\")]\n",
    "\n",
    "    # Reset index to clean up levels after filtering\n",
    "    telco.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Drop additional columns\n",
    "    telco.drop(columns=['PhoneService', 'TotalCharges'], inplace=True)\n",
    "\n",
    "    telco['SeniorCitizen'] = telco['SeniorCitizen'].apply(lambda x: 'True' if x == 1 else 'False')\n",
    "\n",
    "    # Convert object columns to categorical\n",
    "    for col in telco.select_dtypes(include='object').columns:\n",
    "        telco[col] = telco[col].astype('category')\n",
    "\n",
    "    # Perform one-hot encoding, dropping the first category in each variable\n",
    "    telco_encoded = pd.get_dummies(telco, drop_first=True)\n",
    "\n",
    "    return telco_encoded\n",
    "\n",
    "# Load the data\n",
    "telco = load_data()\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(telco.head())\n",
    "\n",
    "# Display summary statistics\n",
    "print(telco.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prob 1a\n",
    "# Filter rows where 'Churn' is 'Yes'\n",
    "Churned = telco[telco['Churn_Yes'] == True]\n",
    "\n",
    "# Calculate the average monthly charges for the resulting 'Churned' DataFrame\n",
    "average_monthly_charges = Churned['MonthlyCharges'].mean()\n",
    "\n",
    "# Print the average monthly charges\n",
    "print(f\"Average Monthly Charges for Churned Customers: ${round(average_monthly_charges,2)}\")\n",
    "\n",
    "# Assume a 12.5% profit margin\n",
    "profit_margin = 0.125\n",
    "\n",
    "# Calculate the expected cost of losing one customer (Cost 1)\n",
    "cost_1 = average_monthly_charges * profit_margin\n",
    "\n",
    "# Print the expected cost of losing one customer\n",
    "print(f\"Expected Cost of Losing One Customer (Cost 1): ${round(cost_1,2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prob 1b\n",
    "current_customer = telco[telco['Churn_Yes'] == False]\n",
    "\n",
    "# Calculate the average monthly charges for the resulting 'current customers' DataFrame\n",
    "current_customer_average_monthly_charges = current_customer['MonthlyCharges'].mean()\n",
    "\n",
    "# Print the average monthly charges\n",
    "print(f\"Average Monthly Charges for current Customers: ${round(current_customer_average_monthly_charges,2)}\")\n",
    "\n",
    "# Assume a 12.5% profit margin\n",
    "profit_margin_retained_customers = 0.075\n",
    "\n",
    "# Calculate the expected cost of losing one customer (Cost 1)\n",
    "cost_2 = current_customer_average_monthly_charges * profit_margin_retained_customers\n",
    "\n",
    "# Print the expected cost of losing one customer\n",
    "print(f\"Expected Cost of Losing One Customer (Cost 2): ${round(cost_2,2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:              Churn_Yes   No. Observations:                 4835\n",
      "Model:                            GLM   Df Residuals:                     4817\n",
      "Model Family:                Binomial   Df Model:                           17\n",
      "Link Function:                  Logit   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:                -2328.9\n",
      "Date:                Thu, 21 Nov 2024   Deviance:                       4657.8\n",
      "Time:                        22:46:58   Pearson chi2:                 5.08e+03\n",
      "No. Iterations:                     6   Pseudo R-squ. (CS):             0.2608\n",
      "Covariance Type:            nonrobust                                         \n",
      "===============================================================================================\n",
      "                                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------------------\n",
      "const                           0.6915      1.498      0.462      0.644      -2.244       3.627\n",
      "tenure                         -0.0357      0.003    -13.806      0.000      -0.041      -0.031\n",
      "MonthlyCharges                 -0.0285      0.033     -0.858      0.391      -0.093       0.037\n",
      "gender_Male                    -0.0403      0.072     -0.559      0.576      -0.181       0.101\n",
      "SeniorCitizen_True              0.2102      0.090      2.342      0.019       0.034       0.386\n",
      "Partner_Yes                    -0.0483      0.085     -0.568      0.570      -0.215       0.118\n",
      "Dependents_Yes                 -0.0715      0.100     -0.715      0.475      -0.267       0.124\n",
      "MultipleLines_Yes               0.4799      0.185      2.593      0.010       0.117       0.843\n",
      "InternetService_Fiber optic     1.7121      0.834      2.052      0.040       0.077       3.348\n",
      "OnlineSecurity_Yes             -0.2402      0.188     -1.275      0.202      -0.609       0.129\n",
      "OnlineBackup_Yes               -0.0002      0.184     -0.001      0.999      -0.360       0.360\n",
      "DeviceProtection_Yes            0.1079      0.185      0.582      0.561      -0.256       0.471\n",
      "TechSupport_Yes                -0.1959      0.189     -1.036      0.300      -0.567       0.175\n",
      "StreamingTV_Yes                 0.5501      0.341      1.612      0.107      -0.119       1.219\n",
      "StreamingMovies_Yes             0.5484      0.342      1.603      0.109      -0.122       1.219\n",
      "Contract_One year              -0.5287      0.120     -4.407      0.000      -0.764      -0.294\n",
      "Contract_Two year              -1.1417      0.197     -5.795      0.000      -1.528      -0.756\n",
      "PaperlessBilling_Yes            0.3375      0.084      4.020      0.000       0.173       0.502\n",
      "===============================================================================================\n",
      "           Pred Neg-  Pred Pos+\n",
      "True Neg-       2770        479\n",
      "True Pos+        651        935\n"
     ]
    }
   ],
   "source": [
    "# Prob 1c\n",
    "\n",
    "# Convert boolean columns to integers (0 and 1)\n",
    "bool_columns = telco.select_dtypes(include=['bool']).columns\n",
    "telco[bool_columns] = telco[bool_columns].astype(int)\n",
    "\n",
    "# Define predictor variables and response variable\n",
    "X = telco.drop(columns=['Churn_Yes'])  # Predictor variables (all columns except 'Churn')\n",
    "y = telco['Churn_Yes']  # Response variable\n",
    "\n",
    "# Add a constant to the predictor variables (intercept term)\n",
    "X_with_intercept = sm.add_constant(X)\n",
    "\n",
    "# Fit the logistic regression model\n",
    "logit_model = sm.GLM(y, X_with_intercept, family=sm.families.Binomial()).fit()\n",
    "\n",
    "# Print the summary of the model\n",
    "print(logit_model.summary())\n",
    "\n",
    "ypred = (logit_model.predict(X_with_intercept) > 0.5)\n",
    "\n",
    "# Calculate false negatives (FN) and false positives (FP)\n",
    "FN = ((y == True) & (ypred == False)).sum()\n",
    "FP = ((y == False) & (ypred == True)).sum()\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y, ypred)\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix, index=['True Neg-', 'True Pos+'], columns=['Pred Neg-', 'Pred Pos+'])\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(conf_matrix_df)\n",
    "\n",
    "# Loss=Cost 1∗FN+Cost 2∗FP\n",
    "# I disagree with Cost 2. It should be 5%, since that is the lost revenue due to the unecessary discount. 7.5% is the profit margin of the retain customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prob 1d\n",
    "def calculate_loss(y_pred_i, y):\n",
    "    FN = ((y == True) & (y_pred_i == False)).sum()\n",
    "    FP = ((y == False) & (y_pred_i == True)).sum()\n",
    "    return cost_1*FN + cost_2*FP\n",
    "\n",
    "# Calculate loss for 1000 evenly distributed thresholds\n",
    "thresholds = np.arange(0.0, 1.0, .001)\n",
    "losses = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_i = (logit_model.predict(X_with_intercept) > threshold)\n",
    "    loss = calculate_loss(y_pred_i, y)\n",
    "    losses.append(loss)\n",
    "\n",
    "print(losses)\n",
    "# Find the optimal threshold\n",
    "optimal_threshold = thresholds[np.argmin(losses)]\n",
    "print(f\"Optimal Threshold: {optimal_threshold}\")\n",
    "\n",
    "# Plot the loss with respect to the threshold\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, losses, marker='o', markersize=2)\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Loss (FN + FP)')\n",
    "plt.title('Loss with Respect to Threshold')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 4 import libaries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prob 4 explore data\n",
    "\n",
    "letters = pd.read_csv('letters-2.csv')\n",
    "print(letters.dtypes)\n",
    "print(letters.head())\n",
    "\n",
    "# Display summary statistics\n",
    "print(letters.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 Prepar X and Y\n",
    "# Split data\n",
    "letters_shuffled = letters.copy().sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "train_size = int(0.75 * len(letters_shuffled))\n",
    "train_set = letters_shuffled[:train_size]\n",
    "test_set = letters_shuffled[train_size:]\n",
    "\n",
    "# Display the sizes of the training and testing sets\n",
    "print(f\"Training set size: {train_set.shape[0]}\")\n",
    "print(f\"Testing set size: {test_set.shape[0]}\")\n",
    "\n",
    "# Define predictor variables and response variable\n",
    "X_train = train_set.drop(columns=['lettr'])  # Predictor variables (all columns except 'lettr')\n",
    "y_train = train_set['lettr']  # Response variable\n",
    "y_train_numeric = y_train.astype('category').cat.codes  # Encode categorical target\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_train_scaled_with_intercept = sm.add_constant(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4A with Stats Model\n",
    "try:\n",
    "    mnlogit_model = sm.MNLogit(y_train_numeric, X_train_scaled_with_intercept).fit()\n",
    "    # Display the model summary\n",
    "    print(mnlogit_model.summary())\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while fitting the model: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4a - SK learn fit model\n",
    "sk_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000, random_state=42)\n",
    "sk_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "coefficients = sk_model.coef_  # Shape: (n_classes, n_features)\n",
    "intercepts = sk_model.intercept_  # Shape: (n_classes,)\n",
    "\n",
    "# Classes and shape details\n",
    "print(f\"Classes: {sk_model.classes_}\")\n",
    "print(f\"Number of features: {X_train_scaled.shape[1]}\")\n",
    "print(f\"Number of classes: {len(sk_model.classes_)}\")\n",
    "\n",
    "# Coefficients\n",
    "print(\"Coefficients (one row per class):\")\n",
    "print(coefficients)\n",
    "\n",
    "# Intercepts\n",
    "print(\"Intercepts (one per class):\")\n",
    "print(intercepts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4b\n",
    "# Predict the letters for the training set\n",
    "train_pred = sk_model.predict(X_train_scaled)\n",
    "\n",
    "# Display the first few predictions\n",
    "print(train_pred[:10])\n",
    "\n",
    "# Create the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_train, train_pred)\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix, index=sk_model.classes_, columns=sk_model.classes_)\n",
    "\n",
    "print(conf_matrix_df)\n",
    "\n",
    "# Calculate classification accuracy by summing the diagonal elements and dividing by the total number of samples\n",
    "correct_classifications = np.trace(conf_matrix)  # Sum of diagonal elements\n",
    "total_classifications = np.sum(conf_matrix)  # Total number of samples\n",
    "accuracy_manual = correct_classifications / total_classifications\n",
    "\n",
    "print(f\"Classification Accuracy (manual calculation): {accuracy_manual:.4f}\")\n",
    "\n",
    "# Calculate classification accuracy using sklearn's accuracy_score for comparison\n",
    "accuracy_sklearn = accuracy_score(y_train, train_pred)\n",
    "print(f\"Classification Accuracy (sklearn): {accuracy_sklearn:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4c\n",
    "# Define predictor variables and response variable\n",
    "X_test = test_set.drop(columns=['lettr'])  # Predictor variables (all columns except 'lettr')\n",
    "y_test = test_set['lettr']  # Response variable\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n",
    "# Predict the letters for the training set\n",
    "test_pred = sk_model.predict(X_test_scaled)\n",
    "\n",
    "# Display the first few predictions\n",
    "print(test_pred[:10])\n",
    "\n",
    "# Calculate classification accuracy using sklearn's accuracy_score for comparison\n",
    "accuracy_sklearn = accuracy_score(y_test, test_pred)\n",
    "print(f\"Classification Accuracy (sklearn) test: {accuracy_sklearn:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4d\n",
    "\n",
    "# Prepare the full dataset\n",
    "full_set = letters.copy()\n",
    "\n",
    "# Display the sizes of the training and testing sets\n",
    "print(f\"Training set size: {full_set.shape[0]}\")\n",
    "print(f\"Testing set size: {full_set.shape[0]}\")\n",
    "\n",
    "# Define predictor variables and response variable\n",
    "X = full_set.drop(columns=['lettr'])  # Predictor variables (all columns except 'lettr')\n",
    "y = full_set['lettr']  # Response variable\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train a logistic regression model (multinomial)\n",
    "model_full = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000, random_state=42)\n",
    "model_full.fit(X_scaled, y)\n",
    "\n",
    "# Read data and predict\n",
    "top_secret = pd.read_csv('topsecret-1.csv')\n",
    "\n",
    "# Ensure the top_secret dataset has the same columns and preprocess it\n",
    "X_top_secret = top_secret[X.columns]  # Ensure the same columns as X\n",
    "X_top_secret_scaled = scaler.transform(X_top_secret)  # Use the same scaler as training\n",
    "\n",
    "# Predict on the top_secret dataset\n",
    "pred = model_full.predict(X_top_secret_scaled)\n",
    "\n",
    "# Display predictions\n",
    "print(\"Predictions for top_secret dataset:\")\n",
    "print(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 1-----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (a)\n",
    "boo = pd.read_csv(\"boo.csv\")\n",
    "\n",
    "# Less than 1% of rows have one or more missing values; drop them\n",
    "print(f\"Original rows: {len(boo)}\")\n",
    "boo_clean = boo.dropna()\n",
    "print(f\"Cleaned rows: {len(boo_clean)}\")\n",
    "\n",
    "X = boo_clean[['x1', 'x2', 'x3', 'x4', 'x5', 'x6']]\n",
    "y = boo_clean['y']\n",
    "X_with_intercept = sm.add_constant(X)  # Add a constant term to the model\n",
    "\n",
    "model = sm.OLS(y, X_with_intercept).fit()\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "boo_fillednas = boo.fillna(boo.mean())\n",
    "X_fillednas = boo_fillednas[['x1', 'x2', 'x3', 'x4', 'x5', 'x6']]\n",
    "y_fillednas = boo_fillednas['y']\n",
    "X_fillednas_with_intercept = sm.add_constant(X_fillednas)  # Add a cons tant term to the model\n",
    "\n",
    "model_fillednas = sm.OLS(y_fillednas, X_fillednas_with_intercept).fit()\n",
    "print(model_fillednas.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (b)\n",
    "# Get studentized residuals\n",
    "influence = model.get_influence()\n",
    "studentized_residuals = influence.resid_studentized_internal\n",
    "\n",
    "# Generate points for the standard normal distribution\n",
    "normdis_range = np.linspace(min(studentized_residuals), max(studentized_residuals), 100)\n",
    "normdis = norm.pdf(normdis_range, 0, 1)\n",
    "\n",
    "# Plot the standard normal density\n",
    "plt.hist(studentized_residuals,density=True, label='Histogram of Studentized Residuals', bins=15, edgecolor='black')\n",
    "plt.plot(normdis_range, normdis, 'r-', lw=2, label='Standard Normal Density')\n",
    "\n",
    "plt.xlabel('Studentized Residuals')\n",
    "plt.ylabel('Density')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (c)\n",
    "fitted_values = model.fittedvalues\n",
    "residuals = model.resid\n",
    "plt.scatter(fitted_values, residuals, s=3, color='black')\n",
    "plt.xlabel('Fitted values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 2 ----------------------------------------------------------------------------\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import OLSInfluence\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "\n",
    "def plot_diagnostics(results, X, y):\n",
    "    # Create influence instance\n",
    "    influence = OLSInfluence(results)\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "    \n",
    "    # 1. Residuals vs Fitted\n",
    "    axes[0, 0].scatter(results.fittedvalues, results.resid, edgecolors='k', facecolors='none')\n",
    "    axes[0, 0].set_xlabel('Fitted values')\n",
    "    axes[0, 0].set_ylabel('Residuals')\n",
    "    axes[0, 0].set_title('Residuals vs Fitted')\n",
    "    # Add smoothed line of fit\n",
    "    smooth_resid = lowess(results.resid, results.fittedvalues)\n",
    "    axes[0, 0].plot(smooth_resid[:, 0], smooth_resid[:, 1], color='r', lw=2)\n",
    "    \n",
    "    # 2. Normal Q-Q\n",
    "    sm.qqplot(results.resid_pearson, line='45', fit=True, ax=axes[0, 1])\n",
    "    axes[0, 1].set_title('Normal Q-Q')\n",
    "    \n",
    "    # 3. Scale-Location\n",
    "    standardized_resid = results.get_influence().resid_studentized_internal\n",
    "    axes[1, 0].scatter(results.fittedvalues, np.sqrt(np.abs(standardized_resid)), edgecolors='k', facecolors='none')\n",
    "    axes[1, 0].set_xlabel('Fitted values')\n",
    "    axes[1, 0].set_ylabel('$\\\\sqrt{|Standardized residuals|}$')\n",
    "    axes[1, 0].set_title('Scale-Location')\n",
    "    sqrt_abs_resid = np.sqrt(np.abs(standardized_resid))\n",
    "    smooth = lowess(sqrt_abs_resid, results.fittedvalues)\n",
    "    axes[1, 0].plot(smooth[:, 0], smooth[:, 1], color='r', lw=2)\n",
    "    \n",
    "     \n",
    "    # 4. Residuals vs Leverage\n",
    "    axes[1, 1].scatter(influence.hat_matrix_diag, results.resid_pearson, edgecolors='k', facecolors='none')\n",
    "    axes[1, 1].set_xlabel('Leverage')\n",
    "    axes[1, 1].set_ylabel('Standardized residuals')\n",
    "    axes[1, 1].set_title('Residuals vs Leverage')\n",
    "    axes[1, 1].axhline(y=0, color='r', linestyle='--')\n",
    "\n",
    "    # Add Cook's distance contours\n",
    "    cooksx = np.linspace(0.001, max(influence.hat_matrix_diag), 100)\n",
    "    p = len(results.params)\n",
    "    poscooks = np.sqrt((p * (1 - cooksx)) / cooksx)\n",
    "    negcooks = -np.sqrt((p * (1 - cooksx)) / cooksx)\n",
    "\n",
    "    axes[1, 1].plot(cooksx, poscooks, 'r--', lw=1)\n",
    "    axes[1, 1].plot(cooksx, negcooks, 'r--', lw=1)\n",
    "\n",
    "    # Add annotation for Cook's distance\n",
    "    axes[1, 1].annotate(\"Cook's distance\", xy=(max(cooksx), max(poscooks)), \n",
    "                        xytext=(0, 5), textcoords='offset points', \n",
    "                        ha='right', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "nutrition = pd.read_csv('nutrition.csv')\n",
    "# Display summary statistics\n",
    "summary = nutrition.describe()\n",
    "\n",
    "\n",
    "# Fit the SLR model using LinearRegression\n",
    "X = nutrition['age'] # Predictor variable\n",
    "y = nutrition['woh']  # Response variable\n",
    "# Add a constant to the predictor variable (intercept term)\n",
    "X_with_intercept = sm.add_constant(X)\n",
    "X_with_intercept.rename(columns={'const': 'WOH_intercept'}, inplace=True)\n",
    "# Create and fit the model\n",
    "initial_model = sm.OLS(y, X_with_intercept).fit()\n",
    "\n",
    "plot_diagnostics(initial_model, X_with_intercept, y)\n",
    "\n",
    "# Create the new variable age^2\n",
    "nutrition['age2'] = nutrition['age'] ** 2\n",
    "# Define the new predictor variables including age and age^2\n",
    "X_new = nutrition[['age', 'age2']]\n",
    "# Add a constant to the predictor variables (intercept term)\n",
    "X_new_with_intercept = sm.add_constant(X_new)\n",
    "# Create and fit the MLR model using OLS\n",
    "model_new = sm.OLS(y, X_new_with_intercept).fit()\n",
    "plot_diagnostics(model_new, X_new_with_intercept, y)\n",
    "\n",
    "# Using interaction terms (GROUP)\n",
    "# nutrition is already sorted by age\n",
    "nutrition['group'] = [1 if i < 7 else 0 for i in range(len(nutrition))] # group starts at 0\n",
    "# Create interaction terms\n",
    "nutrition['age_group'] = nutrition['age'] * nutrition['group']\n",
    "# Define the new predictor variables including age, group, and the interaction term\n",
    "X_with_interaction = nutrition[['age', 'group', 'age_group']]\n",
    "# Add a constant to the predictor variables (intercept term)\n",
    "X_interaction_with_intercept = sm.add_constant(X_with_interaction)\n",
    "# Create and fit the interaction model using OLS\n",
    "model_interaction = sm.OLS(y, X_interaction_with_intercept).fit()\n",
    "# Call the function to plot the regression and residuals for the interaction model\n",
    "plot_diagnostics(model_interaction, X_interaction_with_intercept, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 3 -----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data set for problem 3 & display rows\n",
    "cheese = pd.read_csv('cheese.csv')\n",
    "print(cheese.head())  # Display first few rows\n",
    "\n",
    "summary = cheese.describe() \n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part A \n",
    "# Define predictor and response variables\n",
    "display = cheese['disp']  # Predictor variable (in-store display)\n",
    "sales = cheese['vol']     # Response variable (sales volume)\n",
    "log_sales = np.log(sales) # Log-transformed sales volume\n",
    "\n",
    "# Convert display to a categorical variable\n",
    "display_category = display.astype('category')\n",
    "\n",
    "# Add a constant to the predictor variable (intercept term)\n",
    "X_with_intercept = sm.add_constant(display_category)\n",
    "X_with_intercept.rename(columns={'const': 'sales_intercept', 'disp': 'display_in_store'}, inplace=True)\n",
    "\n",
    "# Create and fit the model\n",
    "model = sm.OLS(log_sales, X_with_intercept).fit()\n",
    "\n",
    "# Print the summary of the model\n",
    "print(model.summary())\n",
    "# Plot residuals to check for patterns\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(model.fittedvalues, model.resid)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Fitted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot')\n",
    "plt.show()\n",
    "\n",
    "# Create a box plot to compare log sales for the two groups (with and without in-store displays)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x=display, y=log_sales)\n",
    "plt.xlabel('In-Store Display (0 = No, 1 = Yes)')\n",
    "plt.ylabel('Log Sales Volume')\n",
    "plt.title('Box Plot of Log Sales Volume by In-Store Display')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part b \n",
    "# Define predictor and response variables\n",
    "display = cheese['disp']  # Predictor variable (in-store display)\n",
    "sales = cheese['vol']     # Response variable (sales volume)\n",
    "ln_sales = np.log(sales) # Log-transformed sales volume\n",
    "price = cheese['price']\n",
    "ln_price = np.log(price) # Log-transformed price\n",
    "\n",
    "\n",
    "# Convert display to a categorical variable\n",
    "display_category = display.astype('category')\n",
    "price_display_interaction = display * ln_price\n",
    "\n",
    "# New predictor variables\n",
    "X = pd.DataFrame({\n",
    "    'ln_price': ln_price,\n",
    "    'display': display,\n",
    "    'price_display_interaction': price_display_interaction\n",
    "})\n",
    "\n",
    "# Add a constant to the predictor variables (intercept term)\n",
    "X_with_intercept = sm.add_constant(X)\n",
    "\n",
    "# Create and fit the model\n",
    "model = sm.OLS(log_sales, X_with_intercept).fit()\n",
    "\n",
    "# Print the summary of the model to see the results\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 4-----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan, linear_rainbow\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from scipy.stats import shapiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory & Display Rows\n",
    "stock_data = pd.read_csv(\"mfunds.csv\")\n",
    "print(stock_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part (a): Define variables, Find Excess Return, And Run Model\n",
    "data = stock_data\n",
    "windsor = data['windsor']\n",
    "valmrkt = data['valmrkt']\n",
    "tbill = data['tbill']\n",
    "\n",
    "# Adjust returns by subtracting the risk-free rate\n",
    "windsor_excess = windsor - tbill\n",
    "valmrkt_excess = valmrkt - tbill\n",
    "\n",
    "#Run CAPM Model\n",
    "X = sm.add_constant(valmrkt_excess)  # Adding a constant for the intercept\n",
    "y = windsor_excess\n",
    "model = sm.OLS(y, X).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting diagnostic plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Residuals vs Fitted\n",
    "sns.residplot(x=model.fittedvalues, y=model.resid, lowess=True, ax=axes[0, 0], line_kws={'color': 'red'})\n",
    "axes[0, 0].set_title('Residuals vs Fitted')\n",
    "axes[0, 0].set_xlabel('Fitted values')\n",
    "axes[0, 0].set_ylabel('Residuals')\n",
    "\n",
    "# QQ Plot\n",
    "sm.qqplot(model.get_influence().resid_studentized_internal, line='45', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Normal Q-Q')\n",
    "axes[0, 1].set_ylabel('Standardized Residuals')\n",
    "\n",
    "# Scale-Location Plot\n",
    "sns.scatterplot(x=model.fittedvalues, y=np.sqrt(np.abs(model.get_influence().resid_studentized_internal)), ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Scale-Location')\n",
    "axes[1, 0].set_xlabel('Fitted values')\n",
    "axes[1, 0].set_ylabel('Sqrt(|Standardized Residuals|)')\n",
    "\n",
    "# Residuals vs Leverage\n",
    "sm.graphics.plot_leverage_resid2(model, ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Residuals vs Leverage')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Influence Plot to identify influential observations\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sm.graphics.influence_plot(model, ax=ax, criterion=\"cooks\")\n",
    "ax.set_title('Influence Plot')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (c) Execute the Shapiro-Wilk test for normality\n",
    "shapiro_test_stat, shapiro_p_value = shapiro(model.resid)\n",
    "print(f\"Shapiro-Wilk test statistic: {shapiro_test_stat}, p-value: {shapiro_p_value}\")\n",
    "\n",
    "if shapiro_p_value < 0.05:\n",
    "    print(\"Reject the null hypothesis: Residuals are not normally distributed.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: Residuals are normally distributed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (d) Execute a test for heteroskedasticity (Breusch-Pagan test)\n",
    "bp_test = het_breuschpagan(model.resid, X)\n",
    "bp_p_value = bp_test[3]\n",
    "\n",
    "print(f\"Breusch-Pagan test p-value: {bp_p_value}\")\n",
    "\n",
    "if bp_p_value < 0.05:\n",
    "    print(\"Reject the null hypothesis: There is heteroskedasticity.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: No evidence of heteroskedasticity.\")\n",
    "\n",
    "# Calculating robust standard errors\n",
    "robust_model = model.get_robustcov_results()\n",
    "print(\"Robust Standard Errors:\")\n",
    "print(robust_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (e) Run the CAPM model on all other funds one-by-one\n",
    "fund_columns = data.columns.drop(['valmrkt', 'tbill'])  # Assuming all other columns are funds\n",
    "cooks_distances = {}\n",
    "\n",
    "for fund in fund_columns:\n",
    "    y_fund = data[fund] - tbill\n",
    "    model_fund = sm.OLS(y_fund, X).fit()\n",
    "    influence = model_fund.get_influence()\n",
    "    cooks_d = influence.cooks_distance[0]\n",
    "    max_cooks_idx = np.argmax(cooks_d)\n",
    "    cooks_distances[fund] = (max_cooks_idx, cooks_d[max_cooks_idx])\n",
    "\n",
    "    print(f\"{fund}: Highest Cook's distance at index {max_cooks_idx} with value {cooks_d[max_cooks_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libarys for prob 5\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Load the trades data\n",
    "trades = pd.read_csv(\"trade.csv\")\n",
    "print('Number of observations:', len(trades))\n",
    "print(trades.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape data to have a single country column so that we can count\n",
    "melted_data = trades.melt(id_vars=['lvalue'], value_vars=['ccode1', 'ccode2'], value_name='country')\n",
    "\n",
    "# Sum the trade volume (lvalue) for each country and find the one with the max amount\n",
    "trade_volume_by_country = melted_data.groupby('country')['lvalue'].sum().sort_values(ascending=False)\n",
    "\n",
    "country_highest_volume = trade_volume_by_country.idxmax()\n",
    "max_trade_volume = trade_volume_by_country.max()\n",
    "print(trade_volume_by_country.head(5))\n",
    "print(\"Country with the highest total trade volume:\", country_highest_volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dependent variable and control variables\n",
    "#X = trades[['lrgdp', 'lrgdpcc', 'ldist', 'cu', 'regional', 'comlang', 'border', 'cont1', 'cont2', 'ccode1', 'ccode2']]\n",
    "# Convert categorical variables to dummy variables\n",
    "X = pd.get_dummies(trades[['lrgdp', 'lrgdpcc', 'ldist', 'cu', 'regional', 'comlang', 'border']], drop_first=True)\n",
    "# Convert boolean columns to integers (0 and 1)\n",
    "X = X.astype({col: 'int64' for col in X.select_dtypes(include='bool').columns})\n",
    "\n",
    "X = X.apply(pd.to_numeric, errors='coerce')\n",
    "X = sm.add_constant(X)  # Add a constant term for the intercept\n",
    "y = trades['lvalue']\n",
    "# Run the OLS regression\n",
    "model = sm.OLS(y, X).fit()\n",
    "print(model.summary())\n",
    "\n",
    "X = trades[['cu', 'ldist', 'lrgdpcc', 'lrgdp', 'regional', 'comlang', 'border']]\n",
    "y = trades['lvalue']\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Run the OLS regression\n",
    "model = sm.OLS(y, X).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate VIF for each feature\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['Variable'] = X.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cluster information\n",
    "trades['group'] = trades['ccode1'].astype(str) + \"_\" + trades['ccode2'].astype(str)\n",
    "\n",
    "# Fit the model with clustered standard errors\n",
    "clustered_fit = model.get_robustcov_results(cov_type='cluster', groups=trades['group'])\n",
    "\n",
    "# Display the summary of the regression results\n",
    "print(clustered_fit.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get leverage and residuals\n",
    "influence = model.get_influence()\n",
    "leverage = influence.hat_matrix_diag\n",
    "residuals = influence.resid_studentized_external\n",
    "\n",
    "# Create a leverage plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(leverage, residuals, alpha=0.5)\n",
    "plt.axhline(0, linestyle='--', color='red', linewidth=2)\n",
    "plt.axvline(0.04, linestyle='--', color='green', linewidth=2, label='High Leverage Cutoff (0.04)')\n",
    "plt.xlabel('Leverage')\n",
    "plt.ylabel('Studentized Residuals')\n",
    "plt.title('Leverage vs. Studentized Residuals')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "high_leverage_indices = trades[leverage > 0.04].index\n",
    "high_leverage_countries = trades.iloc[high_leverage_indices]\n",
    "print(high_leverage_countries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
